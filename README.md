# ğŸ¤– Auto Reinforcement Learning Framework

Um framework completo e avanÃ§ado para otimizaÃ§Ã£o automÃ¡tica de algoritmos de Reinforcement Learning, com interface web moderna, algoritmos evolutivos para tuning de hiperparÃ¢metros e anÃ¡lise detalhada de resultados.

## ğŸš€ CaracterÃ­sticas Principais

- **ğŸ¯ Algoritmos de RL AvanÃ§ados**: Epsilon-Greedy, UCB1, Thompson Sampling
- **ğŸ§¬ Otimizadores Evolutivos**: Algoritmo GenÃ©tico, PSO, Differential Evolution
- **ğŸŒ Interface Web Moderna**: AplicaÃ§Ã£o Flask com dashboard avanÃ§ado
- **ğŸ“Š MÃ©tricas Detalhadas**: AnÃ¡lise completa de treinamento e otimizaÃ§Ã£o
- **ğŸ”§ Auto RL**: OtimizaÃ§Ã£o automÃ¡tica de hiperparÃ¢metros
- **ğŸ“ˆ VisualizaÃ§Ãµes Interativas**: GrÃ¡ficos mÃºltiplos e relatÃ³rios
- **ğŸ“„ RelatÃ³rios Completos**: Download de resultados e anÃ¡lises
- **ğŸ¨ UI/UX Profissional**: Interface responsiva e intuitiva

## ğŸ“ Estrutura do Projeto

```
autorl_framework/
â”œâ”€â”€ src/autorl_framework/
â”‚   â”œâ”€â”€ rl_algs/           # Algoritmos de Reinforcement Learning
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ epsilon_greedy.py
â”‚   â”‚   â”œâ”€â”€ ucb.py
â”‚   â”‚   â””â”€â”€ thompson_sampling.py
â”‚   â”œâ”€â”€ optimizers/        # Algoritmos de otimizaÃ§Ã£o evolutiva
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ genetic_algorithm.py
â”‚   â”‚   â”œâ”€â”€ particle_swarm.py
â”‚   â”‚   â””â”€â”€ differential_evolution.py
â”‚   â””â”€â”€ simulation/        # SimulaÃ§Ã£o e engine principal
â”‚       â”œâ”€â”€ simulator.py
â”‚       â””â”€â”€ auto_rl_engine.py
â”œâ”€â”€ templates/             # Templates HTML da interface web
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ static/               # Arquivos estÃ¡ticos (CSS, JS)
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js
â”œâ”€â”€ data/                 # Datasets de exemplo
â”‚   â”œâ”€â”€ control_group.csv
â”‚   â””â”€â”€ test_group.csv
â”œâ”€â”€ web_app.py           # AplicaÃ§Ã£o Flask principal
â”œâ”€â”€ main.py              # Script de linha de comando
â”œâ”€â”€ test_evolutionary_optimization.py  # Testes do framework
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.12+
- pip ou poetry

### InstalaÃ§Ã£o RÃ¡pida

```bash
# Clonar o repositÃ³rio
git clone <repository-url>
cd autorl_framework

# Criar ambiente virtual
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### VerificaÃ§Ã£o da InstalaÃ§Ã£o

```bash
# Verificar se tudo estÃ¡ funcionando
python check_install.py
```

## ğŸš€ Como Usar

### Interface Web (Recomendado)

```bash
# Ativar ambiente virtual
source venv/bin/activate

# Executar aplicaÃ§Ã£o web
python web_app.py
```

Acesse: **http://localhost:8080**

### Funcionalidades da Interface Web

#### ğŸ“Š **Gerenciamento de Dados de Entrada**
- Upload de arquivos CSV customizados
- Dados de exemplo (campanhas de marketing)
- PrÃ©via dos dados antes da execuÃ§Ã£o

#### âš™ï¸ **ConfiguraÃ§Ã£o do Experimento**
- SeleÃ§Ã£o de algoritmos RL com descriÃ§Ãµes detalhadas
- Otimizadores evolutivos configurÃ¡veis
- Ajuste de geraÃ§Ãµes e iteraÃ§Ãµes

#### ğŸ“‹ **ConfiguraÃ§Ã£o do Experimento Executado**
- Resumo completo dos parÃ¢metros utilizados
- Fonte de dados e mÃ©tricas de recompensa
- Objetivo de otimizaÃ§Ã£o

#### ğŸ“Š **EstatÃ­sticas dos Dados**
- Total de campanhas e dias de dados
- Gasto total e compras mÃ©dias
- ROI mÃ©dio e taxa de conversÃ£o

#### ğŸ“ˆ **MÃ©tricas de Treinamento**
- Total de execuÃ§Ãµes vs bem-sucedidas vs falharam
- Taxa de convergÃªncia
- Desvio padrÃ£o de recompensas e arrependimentos
- Melhorias obtidas na otimizaÃ§Ã£o

#### ğŸ”§ **HiperparÃ¢metros Otimizados**
- Melhores parÃ¢metros encontrados
- Detalhes da melhor e pior execuÃ§Ã£o
- Valores de fitness e performance

#### ğŸ“Š **Resultados e VisualizaÃ§Ãµes**
- **4 GrÃ¡ficos Interativos**:
  - EvoluÃ§Ã£o das Recompensas
  - EvoluÃ§Ã£o do Arrependimento
  - DistribuiÃ§Ã£o de Recompensas
  - CorrelaÃ§Ã£o: Recompensa vs Arrependimento
- Tabela detalhada de resultados
- Download de CSV e relatÃ³rios completos

### Script de Linha de Comando

```bash
# Executar framework via script
python main.py
```

## ğŸ“Š Algoritmos Implementados

### ğŸ¯ Algoritmos de Multi-Armed Bandit

- **Epsilon-Greedy**: ExploraÃ§Ã£o vs explotaÃ§Ã£o balanceada
  - HiperparÃ¢metro: `epsilon` (taxa de exploraÃ§Ã£o)
- **UCB1**: Upper Confidence Bound
  - HiperparÃ¢metro: `alpha` (fator de exploraÃ§Ã£o)
- **Thompson Sampling**: Amostragem bayesiana
  - HiperparÃ¢metro: `prior_alpha`, `prior_beta` (priors)

### ğŸ§¬ Otimizadores Evolutivos

- **Algoritmo GenÃ©tico**: SeleÃ§Ã£o natural
  - PopulaÃ§Ã£o, taxa de mutaÃ§Ã£o, taxa de crossover
- **PSO**: Particle Swarm Optimization
  - InÃ©rcia, cogniÃ§Ã£o, social
- **Differential Evolution**: EvoluÃ§Ã£o diferencial
  - Fator de diferenciaÃ§Ã£o, taxa de crossover

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### HiperparÃ¢metros dos Algoritmos

```python
# Exemplo: Epsilon-Greedy
epsilon_greedy_params = {
    'epsilon': (0.01, 0.5)  # Taxa de exploraÃ§Ã£o
}

# Exemplo: UCB1
ucb_params = {
    'alpha': (0.1, 2.0)  # Fator de exploraÃ§Ã£o
}

# Exemplo: Thompson Sampling
thompson_params = {
    'prior_alpha': (0.1, 2.0),
    'prior_beta': (0.1, 2.0)
}
```

### ConfiguraÃ§Ã£o de Otimizadores

```python
# Algoritmo GenÃ©tico
ga_config = {
    'population_size': 50,
    'mutation_rate': 0.1,
    'crossover_rate': 0.8,
    'n_generations': 20
}

# PSO
pso_config = {
    'n_particles': 30,
    'inertia': 0.7,
    'cognition': 1.5,
    'social': 1.5
}
```

## ğŸ“ˆ Exemplos de Uso

### OtimizaÃ§Ã£o Individual

```python
from autorl_framework.simulation.auto_rl_engine import AutoRLEngine
import pandas as pd

# Carregar dados
df_control = pd.read_csv('data/control_group.csv', sep=';')
df_test = pd.read_csv('data/test_group.csv', sep=';')

# Preparar dados
df_full = pd.concat([df_control, df_test], ignore_index=True)

# Criar engine
engine = AutoRLEngine(
    data=df_full, 
    arms_col='campaign_name', 
    reward_col='purchases_reward'
)

# Otimizar Epsilon-Greedy com algoritmo genÃ©tico
results = engine.run_comparison(
    algorithms=['epsilon_greedy'],
    optimizers=['genetic'],
    n_iterations=3
)

print(f"Melhor recompensa: {results['total_reward'].max()}")
print(f"Melhores parÃ¢metros: {results.loc[results['total_reward'].idxmax(), 'best_params']}")
```

### ComparaÃ§Ã£o AutomÃ¡tica

```python
# Comparar mÃºltiplos algoritmos e otimizadores
results = engine.run_comparison(
    algorithms=['epsilon_greedy', 'ucb', 'thompson'],
    optimizers=['genetic', 'pso', 'differential_evolution'],
    n_iterations=5
)

# AnÃ¡lise dos resultados
print("Resultados por algoritmo:")
for alg in results['algorithm'].unique():
    alg_results = results[results['algorithm'] == alg]
    print(f"{alg}: {alg_results['total_reward'].mean():.2f} Â± {alg_results['total_reward'].std():.2f}")
```

## ğŸ“Š MÃ©tricas e AnÃ¡lises

### MÃ©tricas de Performance

- **Recompensa Total**: Soma das recompensas obtidas
- **Recompensa MÃ©dia**: MÃ©dia por iteraÃ§Ã£o
- **Arrependimento Total**: DiferenÃ§a para o Ã³timo
- **Taxa de ConvergÃªncia**: % de execuÃ§Ãµes acima da mÃ©dia
- **Desvio PadrÃ£o**: Variabilidade dos resultados

### MÃ©tricas de Dados

- **Total de Campanhas**: NÃºmero de arms disponÃ­veis
- **Total de Dias**: PerÃ­odo de dados
- **Gasto Total**: Investimento total
- **ROI MÃ©dio**: Return on Investment
- **Taxa de ConversÃ£o**: Efetividade das campanhas

### VisualizaÃ§Ãµes Geradas

1. **EvoluÃ§Ã£o das Recompensas**: Linha temporal com preenchimento
2. **EvoluÃ§Ã£o do Arrependimento**: Linha temporal com preenchimento
3. **DistribuiÃ§Ã£o de Recompensas**: GrÃ¡fico de barras por iteraÃ§Ã£o
4. **CorrelaÃ§Ã£o Recompensa vs Arrependimento**: Scatter plot

## ğŸ“„ RelatÃ³rios e ExportaÃ§Ã£o

### Download de Resultados

- **CSV dos Resultados**: Tabela completa com todas as mÃ©tricas
- **RelatÃ³rio Completo**: Arquivo .txt com anÃ¡lise detalhada
- **GrÃ¡ficos Interativos**: VisualizaÃ§Ãµes no navegador

### Estrutura do RelatÃ³rio

```
============================================================
RELATÃ“RIO DE EXPERIMENTO AUTORL
============================================================

CONFIGURAÃ‡ÃƒO DO EXPERIMENTO:
- Algoritmo: Epsilon-Greedy (ExploraÃ§Ã£o vs ExploraÃ§Ã£o)
- Otimizador: Algoritmo GenÃ©tico (SeleÃ§Ã£o Natural)
- GeraÃ§Ãµes: 20
- IteraÃ§Ãµes: 3

ESTATÃSTICAS DOS DADOS:
- Total de Campanhas: 2
- Total de Dias: 30
- Gasto Total: $123,456
- Compras MÃ©dias: 45.67

RESULTADOS:
- Total de ExecuÃ§Ãµes: 3
- Recompensa MÃ©dia: 17,306.10
- Melhor Recompensa: 17,360.90
- Arrependimento MÃ©dio: 1,234.56

DETALHES DAS EXECUÃ‡Ã•ES:
IteraÃ§Ã£o 1:
  Recompensa: 17,306.10
  Arrependimento: 1,234.56
  ParÃ¢metros: {"epsilon": 0.2163}
...
```

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Executar Testes

```bash
# Teste completo do framework
python test_evolutionary_optimization.py

# Teste de melhorias
python test_improvements.py
```

### VerificaÃ§Ã£o de Funcionamento

```bash
# Verificar instalaÃ§Ã£o
python check_install.py

# Executar testes unitÃ¡rios
python -m pytest tests/
```

## ğŸš€ Desenvolvimento

### Estrutura Modular

O framework foi projetado para ser facilmente extensÃ­vel:

- **Novos Algoritmos**: Implementar `BaseRLAlgorithm`
- **Novos Otimizadores**: Implementar `BaseOptimizer`
- **Novas MÃ©tricas**: Adicionar ao `AutoRLEngine`

### Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Implemente suas mudanÃ§as
4. Adicione testes
5. Submeta um pull request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, leia as diretrizes de contribuiÃ§Ã£o antes de submeter pull requests.

## ğŸ“ Suporte

Para dÃºvidas, problemas ou sugestÃµes:

1. Abra uma issue no GitHub
2. Consulte a documentaÃ§Ã£o
3. Execute os testes para verificar a instalaÃ§Ã£o

---

**ğŸ‰ Framework AutoRL - OtimizaÃ§Ã£o AutomÃ¡tica de Reinforcement Learning com Interface Web AvanÃ§ada!**
